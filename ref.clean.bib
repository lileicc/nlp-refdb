% Encoding: UTF-8
% date: 2021.6.10

@inproceedings{anastasopoulos2016unsupervised,
 author = {Anastasopoulos, Antonios  and
Chiang, David  and
Duong, Long},
 booktitle = {Proc. of EMNLP},
 pages = {1255--1263},
 title = {An Unsupervised Probability Model for Speech-to-Translation Alignment of Low-Resource Languages},
 year = {2016}
}

@inproceedings{anastasopoulos2018tied,
 author = {Anastasopoulos, Antonios  and
Chiang, David},
 booktitle = {Proc. of NAACL-HLT},
 pages = {82--91},
 title = {Tied Multitask Learning for Neural Speech Translation},
 year = {2018}
}

@inproceedings{baevski2020wav2vec,
 author = {Alexei Baevski and
Yuhao Zhou and
Abdelrahman Mohamed and
Michael Auli},
 booktitle = {NeurIPS},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 title = {wav2vec 2.0: {A} Framework for Self-Supervised Learning of Speech
Representations},
 year = {2020}
}

@InProceedings{bahar2019comparative,
  author       = {Bahar, Parnia and Bieschke, Tobias and Ney, Hermann},
  booktitle    = {Proc. of ASRU},
  title        = {A comparative study on end-to-end speech to text translation},
  year         = {2019},
  organization = {IEEE},
  pages        = {792--799},
}

@inproceedings{bahar2019using,
 author = {Bahar, Parnia and Zeyer, Albert and Schl{\"u}ter, Ralf and Ney, Hermann},
 booktitle = {IWSLT},
 title = {On using specaugment for end-to-end speech translation},
 year = {2019}
}

@inproceedings{bansal2019pre,
 author = {Bansal, Sameer  and
Kamper, Herman  and
Livescu, Karen  and
Lopez, Adam  and
Goldwater, Sharon},
 booktitle = {Proc. of NAACL-HLT},
 pages = {58--68},
 title = {Pre-training on high-resource speech recognition improves low-resource speech-to-text translation},
 year = {2019}
}

@InProceedings{battenberg2017exploring,
  author       = {Battenberg, Eric and Chen, Jitong and Child, Rewon and Coates, Adam and Li, Yashesh Gaur Yi and Liu, Hairong and Satheesh, Sanjeev and Sriram, Anuroop and Zhu, Zhenyao},
  booktitle    = {Proc. of ASRU},
  title        = {Exploring neural transducers for end-to-end speech recognition},
  year         = {2017},
  organization = {IEEE},
  pages        = {206--213},
}

@inproceedings{beck2019neural,
 author = {Beck, Daniel  and
Cohn, Trevor  and
Haffari, Gholamreza},
 booktitle = {Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)},
 pages = {26--31},
 title = {Neural Speech Translation using Lattice Transformations and Graph Networks},
 year = {2019}
}

@inproceedings{berard2016listen,
 author = {B{\'e}rard, Alexandre and Pietquin, Olivier and Servan, Christophe and Besacier, Laurent},
 booktitle = {NIPS workshop on End-to-end Learning for Speech and Audio Processing},
 title = {Listen and translate: A proof of concept for end-to-end speech-to-text translation},
 year = {2016}
}

@inproceedings{berard2018end,
 author = {Alexandre Berard and
Laurent Besacier and
Ali Can Kocabiyikoglu and
Olivier Pietquin},
 booktitle = {ICASSP},
 pages = {6224--6228},
 title = {End-to-End Automatic Speech Translation of Audiobooks},
 year = {2018}
}

@InProceedings{bertoldi2005new,
  author       = {Bertoldi, Nicola and Federico, Marcello},
  booktitle    = {Proc. of ARSU},
  title        = {A new decoder for spoken language translation based on confusion networks},
  year         = {2005},
  organization = {IEEE},
  pages        = {86--91},
}

@inproceedings{besacier2006towards,
 author = {Besacier, Laurent and Zhou, Bowen and Gao, Yuqing},
 booktitle = {Proc. SLT},
 organization = {IEEE},
 pages = {222--225},
 title = {Towards speech translation of non written languages},
 year = {2006}
}

@article{biadsy2019parrotron,
 author = {Biadsy, Fadi and Weiss, Ron J and Moreno, Pedro J and Kanvesky, Dimitri and Jia, Ye},
 journal = {arXiv preprint arXiv:1904.04169},
 title = {Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation},
 year = {2019}
}

@article{binder2003neural,
 author = {Binder, Jeffrey R and McKiernan, Kristen A and Parsons, Melanie E and Westbury, Chris F and Possing, Edward T and Kaufman, Jacqueline N and Buchanan, Lori},
 journal = {Journal of cognitive neuroscience},
 number = {3},
 pages = {372--393},
 title = {Neural correlates of lexical access during visual word recognition},
 volume = {15},
 year = {2003}
}

@article{blank2002speech,
 author = {Blank, S Catrin and Scott, Sophie K and Murphy, Kevin and Warburton, Elizabeth and Wise, Richard JS},
 journal = {Brain},
 number = {8},
 pages = {1829--1838},
 title = {Speech production: Wernicke, Broca and beyond},
 volume = {125},
 year = {2002}
}

@InProceedings{callisonburch2009findings,
  author    = {Callison-Burch, Chris and Koehn, Philipp and Monz, Christof and Schroeder, Josh},
  booktitle = {Proceedings of the Fourth Workshop on Statistical Machine Translation},
  title     = {Findings of the 2009 {W}orkshop on {S}tatistical {M}achine {T}ranslation},
  year      = {2009},
  pages     = {1--28},
}

@inproceedings{bojar2016findings,
 author = {Bojar, Ond{\v{r}}ej  and
Chatterjee, Rajen  and
Federmann, Christian  and
Graham, Yvette  and
Haddow, Barry  and
Huck, Matthias  and
Jimeno Yepes, Antonio  and
Koehn, Philipp  and
Logacheva, Varvara  and
Monz, Christof  and
Negri, Matteo  and
N{\'e}v{\'e}ol, Aur{\'e}lie  and
Neves, Mariana  and
Popel, Martin  and
Post, Matt  and
Rubino, Raphael  and
Scarton, Carolina  and
Specia, Lucia  and
Turchi, Marco  and
Verspoor, Karin  and
Zampieri, Marcos},
 booktitle = {Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
 pages = {131--198},
 title = {Findings of the 2016 Conference on Machine Translation},
 year = {2016}
}

@inproceedings{cettolo2014report,
 author = {Cettolo, Mauro and Niehues, Jan and St{\"u}ker, Sebastian and Bentivogli, Luisa and Federico, Marcello},
 booktitle = {IWSLT},
 title = {Report on the 11th iwslt evaluation campaign, iwslt 2014},
 volume = {57},
 year = {2014}
}

@article{chen2016phone,
 author = {Chen, Zhehuai and Zhuang, Yimeng and Qian, Yanmin and Yu, Kai},
 journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
 number = {1},
 pages = {90--101},
 title = {Phone synchronous speech recognition with ctc lattices},
 volume = {25},
 year = {2016}
}

@inproceedings{cheng2018towards,
 author = {Cheng, Yong  and
Tu, Zhaopeng  and
Meng, Fandong  and
Zhai, Junjie  and
Liu, Yang},
 booktitle = {Proc. of ACL},
 pages = {1756--1766},
 title = {Towards Robust Neural Machine Translation},
 year = {2018}
}

@inproceedings{cheng2019breaking,
 author = {Cheng, Qiao and Fang, Meiyuan and Han, Yaqian and Huang, Jin and Duan, Yitao},
 booktitle = {IWSLT},
 title = {Breaking the Data Barrier: Towards Robust Speech Translation via Adversarial Stability Training},
 year = {2019}
}

@article{collobert2011natural,
 author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
 journal = {JMLR},
 number = {Aug},
 pages = {2493--2537},
 title = {Natural language processing (almost) from scratch},
 volume = {12},
 year = {2011}
}

@inproceedings{dai2019transformer,
 author = {Dai, Zihang  and
Yang, Zhilin  and
Yang, Yiming  and
Carbonell, Jaime  and
Le, Quoc  and
Salakhutdinov, Ruslan},
 booktitle = {Proc. of ACL},
 pages = {2978--2988},
 title = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
 year = {2019}
}

@InProceedings{devlin2019bert,
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proc. of NAACL-HLT},
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year      = {2019},
  pages     = {4171--4186},
}

@InProceedings{digangi2019adapting,
  author       = {Di Gangi, Mattia A and Negri, Matteo and Turchi, Marco},
  booktitle    = {INTERSPEECH},
  title        = {Adapting transformer to end-to-end spoken language translation},
  year         = {2019},
  organization = {International Speech Communication Association (ISCA)},
  pages        = {1133--1137},
}

@InProceedings{digangi2019enhancing,
  author    = {Di Gangi, Mattia Antonino and Negri, Matteo and Cattoni, Roldano and Dessi, Roberto and Turchi, Marco},
  booktitle = {Proceedings of Machine Translation Summit XVII Volume 1: Research Track},
  title     = {Enhancing Transformer for End-to-end Speech-to-Text Translation},
  year      = {2019},
  pages     = {21--31},
}

@InProceedings{digangi2019must,
  author    = {Di Gangi, Mattia A. and Cattoni, Roldano and Bentivogli, Luisa and Negri, Matteo and Turchi, Marco},
  booktitle = {Proc. of NAACL-HLT},
  title     = {{M}u{ST}-{C}: a {M}ultilingual {S}peech {T}ranslation {C}orpus},
  year      = {2019},
  pages     = {2012--2017},
}

@InProceedings{gangi2020instance,
  author    = {Mattia Antonino Di Gangi and Viet{-}Nhat Nguyen and Matteo Negri and Marco Turchi},
  booktitle = {ICASSP},
  title     = {Instance-based Model Adaptation for Direct Speech Translation},
  year      = {2020},
  pages     = {7914--7918},
}

@inproceedings{dixon2011investigation,
 author = {Dixon, Paul R and Finch, Andrew and Hori, Chiori and Kashioka, Hideki},
 booktitle = {IWSLT},
 title = {Investigation on the effects of ASR tuning on speech translation performance},
 year = {2011}
}

@inproceedings{dong2018speech,
 author = {Linhao Dong and
Shuang Xu and
Bo Xu},
 booktitle = {ICASSP},
 pages = {5884--5888},
 title = {Speech-Transformer: {A} No-Recurrence Sequence-to-Sequence Model for
Speech Recognition},
 year = {2018}
}

@inproceedings{dong2021consecutive,
 author = {Dong, Qianqian and Wang, Mingxuan and Zhou, Hao and Xu, Shuang and Xu, Bo and Li, Lei},
 booktitle = {Proc. of AAAI},
 title = {Consecutive Decoding for Speech-to-text Translation},
 year = {2021}
}

@inproceedings{dong2021listen,
 author = {Dong, Qianqian and Ye, Rong and Wang, Mingxuan and Zhou, Hao and Xu, Shuang and Xu, Bo and Li, Lei},
 booktitle = {Proc. of AAAI},
 number = {14},
 pages = {12749--12759},
 title = {Listen, Understand and Translate: Triple Supervision Decouples End-to-end Speech-to-text Translation},
 volume = {35},
 year = {2021}
}

@inproceedings{duong2016attentional,
 author = {Duong, Long  and
Anastasopoulos, Antonios  and
Chiang, David  and
Bird, Steven  and
Cohn, Trevor},
 booktitle = {Proc. of NAACL-HLT},
 pages = {949--959},
 title = {An Attentional Model for Speech Translation Without Transcription},
 year = {2016}
}

@inproceedings{dyer2013simple,
 author = {Dyer, Chris  and
Chahuneau, Victor  and
Smith, Noah A.},
 booktitle = {Proc. of NAACL-HLT},
 pages = {644--648},
 title = {A Simple, Fast, and Effective Reparameterization of {IBM} Model 2},
 year = {2013}
}

@inproceedings{fitzgerald2009reconstructing,
 author = {Fitzgerald, Erin  and
Hall, Keith  and
Jelinek, Frederick},
 booktitle = {Proc. of EACL},
 pages = {255--263},
 title = {Reconstructing False Start Errors in Spontaneous Speech Text},
 year = {2009}
}

@Article{froyen2009long,
  author     = {Froyen, Dries J. W. and Bonte, Milene L. and van Atteveldt, Nienke and Blomert, Leo},
  journal    = {J. Cognitive Neuroscience},
  title      = {The Long Road to Automation: Neurocognitive Development of Letter-Speech Sound Processing},
  year       = {2009},
  issn       = {0898-929X},
  number     = {3},
  pages      = {567â€“580},
  volume     = {21},
  abstract   = {In transparent alphabetic languages, the expected standard for complete acquisition of letter-speech sound associations is within one year of reading instruction. The neural mechanisms underlying the acquisition of letter-speech sound associations have, however, hardly been investigated. The present article describes an ERP study with beginner and advanced readers in which the influence of letters on speech sound processing is investigated by comparing the MMN to speech sounds presented in isolation with the MMN to speech sounds accompanied by letters. Furthermore, SOA between letter and speech sound presentation was manipulated in order to investigate the development of the temporal window of integration for letter-speech sound processing. Beginner readers, despite one year of reading instruction, showed no early letter-speech sound integration, that is, no influence of the letter on the evocation of the MMN to the speech sound. Only later in the difference wave, at 650 msec, was an influence of the letter on speech sound processing revealed. Advanced readers, with 4 years of reading instruction, showed early and automatic letter-speech sound processing as revealed by an enhancement of the MMN amplitude, however, at a different temporal window of integration in comparison with experienced adult readers. The present results indicate a transition from mere association in beginner readers to more automatic, but still not "adult-like," integration in advanced readers. In contrast to general assumptions, the present study provides evidence for an extended development of letter-speech sound integration.},
  issue_date = {March 2009},
  numpages   = {14},
}

@PhdThesis{fuegen2008system,
  author = {F{\"u}gen, Christian},
  school = {Verlag nicht ermittelbar},
  title  = {A system for simultaneous translation of lectures and speeches},
  year   = {2008},
}

@inproceedings{graves2006connectionist,
 author = {Alex Graves and
Santiago Fern{\'{a}}ndez and
Faustino J. Gomez and
J{\"{u}}rgen Schmidhuber},
 booktitle = {Proc. of ICML},
 editor = {William W. Cohen and
Andrew W. Moore},
 pages = {369--376},
 title = {Connectionist temporal classification: labelling unsegmented sequence
data with recurrent neural networks},
 volume = {148},
 year = {2006}
}

@InProceedings{he2011why,
  author       = {He, Xiaodong and Deng, Li and Acero, Alex},
  booktitle    = {ICASSP},
  title        = {Why word error rate is not a good metric for speech recognizer training for the speech translation task?},
  year         = {2011},
  organization = {IEEE},
  pages        = {5632--5635},
}

@InProceedings{inaguma2019multilingual,
  author       = {Inaguma, Hirofumi and Duh, Kevin and Kawahara, Tatsuya and Watanabe, Shinji},
  booktitle    = {Proc. of ASRU},
  title        = {Multilingual end-to-end speech translation},
  year         = {2019},
  organization = {IEEE},
  pages        = {570--577},
}

@inproceedings{inaguma2020espnet,
 author = {Inaguma, Hirofumi  and
Kiyono, Shun  and
Duh, Kevin  and
Karita, Shigeki  and
Yalta, Nelson  and
Hayashi, Tomoki  and
Watanabe, Shinji},
 booktitle = {Proc. of ACL},
 pages = {302--311},
 title = {{ESP}net-{ST}: All-in-One Speech Translation Toolkit},
 year = {2020}
}

@inproceedings{indurthi2020data,
 author = {Indurthi, Sathish and Han, Houjeung and Lakumarapu, Nikhil Kumar and Lee, Beomseok and Chung, Insoo and Kim, Sangha and Kim, Chanwoo},
 booktitle = {ICASSP},
 organization = {IEEE},
 title = {Data efficient direct speech-to-text translation with modality agnostic meta-learning},
 year = {2020}
}

@inproceedings{jan2018iwslt,
 author = {Jan, Niehues and Cattoni, Roldano and Sebastian, St{\"u}ker and Cettolo, Mauro and Turchi, Marco and Federico, Marcello},
 booktitle = {IWSLT},
 pages = {2--6},
 title = {The iwslt 2018 evaluation campaign},
 year = {2018}
}

@InProceedings{jawahar2019what,
  author    = {Jawahar, Ganesh and Sagot, Beno{\^\i}t and Seddah, Djam{\'e}},
  booktitle = {Proc. of ACL},
  title     = {What Does {BERT} Learn about the Structure of Language?},
  year      = {2019},
  pages     = {3651--3657},
}

@InProceedings{jia2019direct,
  author    = {Jia, Ye and Weiss, Ron J and Biadsy, Fadi and Macherey, Wolfgang and Johnson, Melvin and Chen, Zhifeng and Wu, Yonghui},
  booktitle = {Interspeech},
  title     = {Direct speech-to-speech translation with a sequence-to-sequence model},
  year      = {2019},
}

@inproceedings{jia2019leveraging,
 author = {Ye Jia and
Melvin Johnson and
Wolfgang Macherey and
Ron J. Weiss and
Yuan Cao and
Chung{-}Cheng Chiu and
Naveen Ari and
Stella Laurenzo and
Yonghui Wu},
 booktitle = {ICASSP},
 pages = {7180--7184},
 title = {Leveraging Weakly Supervised Data to Improve End-to-end Speech-to-text
Translation},
 year = {2019}
}

@inproceedings{kannan2018analysis,
 author = {Anjuli Kannan and
Yonghui Wu and
Patrick Nguyen and
Tara N. Sainath and
Zhifeng Chen and
Rohit Prabhavalkar},
 booktitle = {ICASSP},
 pages = {5824--5828},
 title = {An Analysis of Incorporating an External Language Model into a Sequence-to-Sequence
Model},
 year = {2018}
}

@InProceedings{kano2017structured,
  author    = {Takatomo Kano and Sakriani Sakti and Satoshi Nakamura},
  booktitle = {INTERSPEECH},
  title     = {Structured-Based Curriculum Learning for End-to-End English-Japanese Speech Translation},
  year      = {2017},
  pages     = {2630--2634},
}

@inproceedings{kiros2015skip,
 author = {Ryan Kiros and
Yukun Zhu and
Ruslan Salakhutdinov and
Richard S. Zemel and
Raquel Urtasun and
Antonio Torralba and
Sanja Fidler},
 booktitle = {NeurIPS},
 editor = {Corinna Cortes and
Neil D. Lawrence and
Daniel D. Lee and
Masashi Sugiyama and
Roman Garnett},
 pages = {3294--3302},
 title = {Skip-Thought Vectors},
 year = {2015}
}

@inproceedings{kocabiyikoglu2018augmenting,
 author = {Kocabiyikoglu, Ali Can  and
Besacier, Laurent  and
Kraif, Olivier},
 booktitle = {LREC},
 title = {Augmenting Librispeech with {F}rench Translations: A Multimodal Corpus for Direct Speech Translation Evaluation},
 year = {2018}
}

@InProceedings{conneau2019cross,
  author    = {Alexis Conneau and Guillaume Lample},
  booktitle = {NeurIPS},
  title     = {Cross-lingual Language Model Pretraining},
  year      = {2019},
  editor    = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
  pages     = {7057--7067},
}

@inproceedings{lavie1996multi,
 author = {Lavie, Alon  and
Gates, Donna  and
Gavalda, Marsal  and
Mayfield, Laura  and
Waibel, Alex  and
Levin, Lori},
 booktitle = {Proc. of COLING},
 title = {Multi-lingual Translation of Spontaneously Spoken Language in a Limited Domain},
 year = {1996}
}

@inproceedings{le2014distributed,
 author = {Quoc V. Le and
Tom{\'{a}}s Mikolov},
 booktitle = {Proc. of ICML},
 pages = {1188--1196},
 series = {{JMLR} Workshop and Conference Proceedings},
 title = {Distributed Representations of Sentences and Documents},
 volume = {32},
 year = {2014}
}

@InProceedings{le2020dual,
  author    = {Le, Hang and Pino, Juan and Wang, Changhan and Gu, Jiatao and Schwab, Didier and Besacier, Laurent},
  booktitle = {Proc. of COLING},
  title     = {Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation},
  year      = {2020},
  pages     = {3520--3533},
}

@inproceedings{lison2016opensubtitles2016,
 author = {Lison, Pierre  and
Tiedemann, J{\"o}rg},
 booktitle = {LREC},
 pages = {923--929},
 title = {{O}pen{S}ubtitles2016: Extracting Large Parallel Corpora from Movie and {TV} Subtitles},
 year = {2016}
}

@inproceedings{liu2003use,
 author = {Liu, Fu-Hua and Gu, Liang and Gao, Yuqing and Picheny, Michael},
 booktitle = {{ICASSP}},
 organization = {IEEE},
 pages = {I--I},
 title = {Use of statistical N-gram models in natural language generation for machine translation},
 volume = {1},
 year = {2003}
}

@article{liu2018ustc,
 author = {Liu, Dan and Liu, Junhua and Guo, Wu and Xiong, Shifu and Ma, Zhiqiang and Song, Rui and Wu, Chongliang and Liu, Quan},
 journal = {arXiv preprint arXiv:1812.02455},
 title = {The USTC-NEL Speech Translation system at IWSLT 2018},
 year = {2018}
}

@inproceedings{liu2019end,
 author = {Yuchen Liu and
Hao Xiong and
Jiajun Zhang and
Zhongjun He and
Hua Wu and
Haifeng Wang and
Chengqing Zong},
 booktitle = {Interspeech},
 editor = {Gernot Kubin and
Zdravko Kacic},
 pages = {1128--1132},
 title = {End-to-End Speech Translation with Knowledge Distillation},
 year = {2019}
}

@article{liu2020bridging,
 author = {Liu, Yuchen and Zhu, Junnan and Zhang, Jiajun and Zong, Chengqing},
 journal = {arXiv preprint arXiv:2010.14920},
 title = {Bridging the Modality Gap for Speech-to-Text Translation},
 year = {2020}
}

@inproceedings{lu2018neural,
 author = {Lu, Yichao  and
Keung, Phillip  and
Ladhak, Faisal  and
Bhardwaj, Vikas  and
Zhang, Shaonan  and
Sun, Jason},
 booktitle = {Proceedings of the Third Conference on Machine Translation: Research Papers},
 pages = {84--92},
 title = {A neural interlingua for multilingual machine translation},
 year = {2018}
}

@InProceedings{lugosch2019speech,
  author    = {Lugosch, Loren and Ravanelli, Mirco and Ignoto, Patrick and Tomar, Vikrant Singh and Bengio, Yoshua},
  booktitle = {Interspeech},
  title     = {Speech Model Pre-training for End-to-End Spoken Language Understanding},
  year      = {2019},
}

@InProceedings{matusov2005phrase,
  author       = {Matusov, Evgeny and Ney, Hermann and Schluter, Ralph},
  booktitle    = {Proc. of ASRU},
  title        = {Phrase-based translation of speech recognizer word lattices using loglinear model combination},
  year         = {2005},
  organization = {IEEE},
  pages        = {110--115},
}

@inproceedings{matusov2006automatic,
 author = {Matusov, Evgeny and Mauser, Arne and Ney, Hermann},
 booktitle = {IWSLT},
 title = {Automatic sentence segmentation and punctuation prediction for spoken language translation},
 year = {2006}
}

@inproceedings{matusov2008spoken,
 author = {Matusov, Evgeny and Hoffmeister, Bj{\"o}rn and Ney, Hermann},
 booktitle = {Interspeech},
 title = {Spoken Language Translation Systems************ ASR Word Lattice Translation with Exhaustive Reordering is Possible},
 year = {2008}
}

@inproceedings{mikolov2013distributed,
 author = {Tom{\'{a}}s Mikolov and
Ilya Sutskever and
Kai Chen and
Gregory S. Corrado and
Jeffrey Dean},
 booktitle = {NeurIPS},
 editor = {Christopher J. C. Burges and
L{\'{e}}on Bottou and
Zoubin Ghahramani and
Kilian Q. Weinberger},
 pages = {3111--3119},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 year = {2013}
}

@InProceedings{ott2019fairseq,
  author    = {Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  booktitle = {Proc. of NAACL - Demonstrations},
  title     = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  year      = {2019},
  pages     = {48--53},
}

@inproceedings{papineni2002bleu,
 author = {Papineni, Kishore  and
Roukos, Salim  and
Ward, Todd  and
Zhu, Wei-Jing},
 booktitle = {Proc. of ACL},
 pages = {311--318},
 title = {{B}leu: a Method for Automatic Evaluation of Machine Translation},
 year = {2002}
}

@InProceedings{park2019specaugment,
  author    = {Park, Daniel S and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D and Le, Quoc V},
  booktitle = {Interspeech},
  title     = {Specaugment: A simple data augmentation method for automatic speech recognition},
  year      = {2019},
}

@inproceedings{peitz2012spoken,
 author = {Peitz, Stephan and Wiesler, Simon and Nu{\ss}baum-Thom, Markus and Ney, Hermann},
 booktitle = {IWSLT},
 title = {Spoken language translation using automatically transcribed text in training},
 year = {2012}
}

@inproceedings{pennington2014glove,
 author = {Pennington, Jeffrey  and
Socher, Richard  and
Manning, Christopher},
 booktitle = {Proc. of EMNLP},
 pages = {1532--1543},
 title = {{G}lo{V}e: Global Vectors for Word Representation},
 year = {2014}
}

@inproceedings{peters2018deep,
 author = {Peters, Matthew  and
Neumann, Mark  and
Iyyer, Mohit  and
Gardner, Matt  and
Clark, Christopher  and
Lee, Kenton  and
Zettlemoyer, Luke},
 booktitle = {Proc. of NAACL-HLT},
 pages = {2227--2237},
 title = {Deep Contextualized Word Representations},
 year = {2018}
}

@inproceedings{pino2019harnessing,
 author = {Pino, Juan and Puzon, Liezl and Gu, Jiatao and Ma, Xutai and McCarthy, Arya D and Gopinath, Deepak},
 booktitle = {IWSLT},
 title = {Harnessing indirect training data for end-to-end automatic speech translation: Tricks of the trade},
 year = {2019}
}

@inproceedings{pino2020self,
 author = {Juan Pino and
Qiantong Xu and
Xutai Ma and
Mohammad Javad Dousti and
Yun Tang},
 booktitle = {{INTERSPEECH}},
 pages = {1476--1480},
 title = {Self-Training for End-to-End Speech Translation},
 year = {2020}
}

@InProceedings{qi2018when,
  author    = {Qi, Ye and Sachan, Devendra and Felix, Matthieu and Padmanabhan, Sarguna and Neubig, Graham},
  booktitle = {Proc. of NAACL-HLT},
  title     = {When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?},
  year      = {2018},
  pages     = {529--535},
}

@article{radford2018improving,
 author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
 journal = {OpenAI Blog},
 title = {Improving language understanding by generative pre-training},
 year = {2018}
}

@article{radford2019language,
 author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
 journal = {OpenAI Blog},
 number = {8},
 title = {Language models are unsupervised multitask learners},
 volume = {1},
 year = {2019}
}

@inproceedings{rousseau2014enhancing,
 author = {Rousseau, Anthony  and
Del{\'e}glise, Paul  and
Est{\`e}ve, Yannick},
 booktitle = {{LREC}},
 pages = {3935--3939},
 title = {Enhancing the {TED}-{LIUM} Corpus with Selected Data for Language Modeling and More {TED} Talks},
 year = {2014}
}

@inproceedings{ruiz2015adapting,
 author = {Ruiz, Nicholas and Gao, Qin and Lewis, William and Federico, Marcello},
 booktitle = {Interspeech},
 title = {Adapting machine translation models toward misrecognized speech with text-to-speech pronunciation rules and acoustic confusability},
 year = {2015}
}

@InProceedings{sak2015fast,
  author    = {Sak, Ha{\c{s}}im and Senior, Andrew and Rao, Kanishka and Beaufays, Fran{\c{c}}oise},
  booktitle = {Interspeech},
  title     = {Fast and accurate recurrent neural network acoustic models for speech recognition},
  year      = {2015},
}

@inproceedings{salazar2019self,
 author = {Julian Salazar and
Katrin Kirchhoff and
Zhiheng Huang},
 booktitle = {ICASSP},
 pages = {7115--7119},
 title = {Self-attention Networks for Connectionist Temporal Classification
in Speech Recognition},
 year = {2019}
}

@inproceedings{salesky2018towards,
 author = {Salesky, Elizabeth and Burger, Susanne and Niehues, Jan and Waibel, Alex},
 booktitle = {2018 (SLT)},
 organization = {IEEE},
 pages = {921--926},
 title = {Towards fluent translations from disfluent speech},
 year = {2018}
}

@inproceedings{salesky2019fluent,
 author = {Salesky, Elizabeth  and
Sperber, Matthias  and
Waibel, Alexander},
 booktitle = {Proc. of NAACL-HLT},
 pages = {2786--2792},
 title = {Fluent Translations from Disfluent Speech in End-to-End Speech Translation},
 year = {2019}
}

@inproceedings{schultz2004using,
 author = {Schultz, Tanja and Jou, Szu-Chen and Vogel, Stephan and Saleem, Shirin},
 booktitle = {Eighth International Conference on Spoken Language Processing},
 title = {Using word latice information for a tighter coupling in speech translation systems},
 year = {2004}
}

@article{shankweiler2008reading,
 author = {Shankweiler, Donald and Mencl, W Einar and Braze, David and Tabor, Whitney and Pugh, Kenneth R and Fulbright, Robert K},
 journal = {Developmental neuropsychology},
 number = {6},
 pages = {745--775},
 title = {Reading differences and brain: Cortical integration of speech and print in sentence processing varies with reader skill},
 volume = {33},
 year = {2008}
}

@inproceedings{socher2013recursive,
 author = {Socher, Richard  and
Perelygin, Alex  and
Wu, Jean  and
Chuang, Jason  and
Manning, Christopher D.  and
Ng, Andrew  and
Potts, Christopher},
 booktitle = {Proc. of EMNLP},
 pages = {1631--1642},
 title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
 year = {2013}
}

@inproceedings{sperber2017neural,
 author = {Sperber, Matthias  and
Neubig, Graham  and
Niehues, Jan  and
Waibel, Alex},
 booktitle = {Proc. of EMNLP},
 pages = {1380--1389},
 title = {Neural Lattice-to-Sequence Models for Uncertain Inputs},
 year = {2017}
}

@InProceedings{sperber2017robust,
  author    = {Sperber, Matthias and Niehues, Jan and Waibel, Alex},
  booktitle = {IWSLT},
  title     = {Toward robust neural machine translation for noisy input sequences},
  year      = {2017},
}

@article{sperber2019attention,
 author = {Sperber, Matthias  and
Neubig, Graham  and
Niehues, Jan  and
Waibel, Alex},
 journal = {TACL},
 pages = {313--325},
 title = {Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation},
 volume = {7},
 year = {2019}
}

@inproceedings{sperber2019self,
 author = {Sperber, Matthias  and
Neubig, Graham  and
Pham, Ngoc-Quan  and
Waibel, Alex},
 booktitle = {Proc. of ACL},
 pages = {1185--1197},
 title = {Self-Attentional Models for Lattice Inputs},
 year = {2019}
}

@article{spitsyna2006converging,
 author = {Spitsyna, Galina and Warren, Jane E and Scott, Sophie K and Turkheimer, Federico E and Wise, Richard JS},
 journal = {Journal of Neuroscience},
 number = {28},
 pages = {7328--7336},
 title = {Converging language streams in the human temporal lobe},
 volume = {26},
 year = {2006}
}

@inproceedings{stoian2020analyzing,
 author = {Mihaela C. Stoian and
Sameer Bansal and
Sharon Goldwater},
 booktitle = {{ICASSP}},
 pages = {7909--7913},
 title = {Analyzing {ASR} Pretraining for Low-Resource Speech-to-Text Translation},
 year = {2020}
}

@InProceedings{sun2020ernie,
  author    = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
  booktitle = {Proc. of AAAI},
  title     = {ERNIE 2.0: A Continual Pre-training Framework for Language Understanding},
  year      = {2020},
}

@inproceedings{sung2019towards,
 author = {Tzu{-}Wei Sung and
Jun{-}You Liu and
Hung{-}yi Lee and
Lin{-}Shan Lee},
 booktitle = {{ICASSP}},
 pages = {7175--7179},
 title = {Towards End-to-end Speech-to-text Translation with Two-pass Decoding},
 year = {2019}
}

@inproceedings{tan2018deep,
 author = {Zhixing Tan and
Mingxuan Wang and
Jun Xie and
Yidong Chen and
Xiaodong Shi},
 booktitle = {Proc. of AAAI},
 editor = {Sheila A. McIlraith and
Kilian Q. Weinberger},
 pages = {4929--4936},
 title = {Deep Semantic Role Labeling With Self-Attention},
 year = {2018}
}

@inproceedings{tang2019understanding,
 author = {Tang, Gongbo  and
Sennrich, Rico  and
Nivre, Joakim},
 booktitle = {Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)},
 pages = {1186--1193},
 title = {Understanding Neural Machine Translation by Simplification: The Case of Encoder-free Models},
 year = {2019}
}

@inproceedings{tang2021general,
 author = {Tang, Yun and Pino, Juan and Wang, Changhan and Ma, Xutai and Genzel, Dmitriy},
 booktitle = {{ICASSP}},
 organization = {IEEE},
 pages = {6209--6213},
 title = {A general multi-task learning framework to leverage text data for speech to text tasks},
 year = {2021}
}

@inproceedings{tsvetkov2014augmenting,
 author = {Tsvetkov, Yulia  and
Metze, Florian  and
Dyer, Chris},
 booktitle = {Proc. of EACL},
 pages = {616--625},
 title = {Augmenting Translation Models with Simulated Acoustic Confusions for Improved Spoken Language Translation},
 year = {2014}
}

@Article{vanatteveldt2004integration,
  author   = {Nienke {van Atteveldt} and Elia Formisano and Rainer Goebel and Leo Blomert},
  journal  = {Neuron},
  title    = {Integration of Letters and Speech Sounds in the Human Brain},
  year     = {2004},
  issn     = {0896-6273},
  number   = {2},
  pages    = {271-282},
  volume   = {43},
  abstract = {Most people acquire literacy skills with remarkable ease, even though the human brain is not evolutionarily adapted to this relatively new cultural phenomenon. Associations between letters and speech sounds form the basis of reading in alphabetic scripts. We investigated the functional neuroanatomy of the integration of letters and speech sounds using functional magnetic resonance imaging (fMRI). Letters and speech sounds were presented unimodally and bimodally in congruent or incongruent combinations. Analysis of single-subject data and group data aligned on the basis of individual cortical anatomy revealed that letters and speech sounds are integrated in heteromodal superior temporal cortex. Interestingly, responses to speech sounds in a modality-specific region of the early auditory cortex were modified by simultaneously presented letters. These results suggest that efficient processing of culturally defined associations between letters and speech sounds relies on neural mechanisms similar to those naturally evolved for integrating audiovisual speech.},
}

@inproceedings{vaswani2017attention,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 booktitle = {NeurIPS},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 title = {Attention is All you Need},
 year = {2017}
}

@inproceedings{vazquez2019multilingual,
 author = {V{\'a}zquez, Ra{\'u}l  and
Raganato, Alessandro  and
Tiedemann, J{\"o}rg  and
Creutz, Mathias},
 booktitle = {Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)},
 pages = {33--39},
 title = {Multilingual {NMT} with a Language-Independent Attention Bridge},
 year = {2019}
}

@inproceedings{vila2018end,
 author = {Vila, Laura Cross and Escolano, Carlos and Fonollosa, Jos{\'e} AR and Costa-juss{\`a}, Marta R},
 booktitle = {IberSPEECH},
 pages = {60--63},
 title = {End-to-End Speech Translation with the Transformer.},
 year = {2018}
}

@inproceedings{vydana2021jointly,
 author = {Vydana, Hari Krishna and Karafi{\'a}t, Martin and Zmolikova, Katerina and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Honza},
 booktitle = {{ICASSP}},
 organization = {IEEE},
 pages = {7513--7517},
 title = {Jointly trained transformers models for spoken language translation},
 year = {2021}
}

@inproceedings{wang2015transfer,
 author = {Wang, Dong and Zheng, Thomas Fang},
 booktitle = {APSIPA},
 organization = {IEEE},
 pages = {1225--1237},
 title = {Transfer learning for speech and language processing},
 year = {2015}
}

@inproceedings{wang2020bridging,
 author = {Wang, Chengyi and Wu, Yu and Liu, Shujie and Yang, Zhenglu and Zhou, Ming},
 booktitle = {Proc. of AAAI},
 number = {05},
 pages = {9161--9168},
 title = {Bridging the gap between pre-training and fine-tuning for end-to-end speech translation},
 volume = {34},
 year = {2020}
}

@inproceedings{wang2020curriculum,
 author = {Wang, Chengyi  and
Wu, Yu  and
Liu, Shujie  and
Zhou, Ming  and
Yang, Zhenglu},
 booktitle = {Proc. of ACL},
 pages = {3728--3738},
 title = {Curriculum Pre-training for End-to-End Speech Translation},
 year = {2020}
}

@inproceedings{weiss2017sequence,
 author = {Ron J. Weiss and
Jan Chorowski and
Navdeep Jaitly and
Yonghui Wu and
Zhifeng Chen},
 booktitle = {{INTERSPEECH}},
 pages = {2625--2629},
 title = {Sequence-to-Sequence Models Can Directly Translate Foreign Speech},
 year = {2017}
}

@inproceedings{woszczyna1993recent,
 author = {Woszczyna, M.  and
Coccaro, N.  and
Eisele, A.  and
Lavie, A.  and
McNair, A.  and
Polzin, T.  and
Rogina, I.  and
Rose, C. P.  and
Sloboda, T.  and
Tomita, M.  and
Tsutsumi, J.  and
Aoki-Waibel, N.  and
Waibel, A.  and
Ward, W.},
 booktitle = {{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993},
 title = {Recent Advances in {J}anus: A Speech Translation System},
 year = {1993}
}

@inproceedings{yang2019improving,
 author = {Yinfei Yang and
Gustavo Hern{\'{a}}ndez {\'{A}}brego and
Steve Yuan and
Mandy Guo and
Qinlan Shen and
Daniel Cer and
Yun{-}Hsuan Sung and
Brian Strope and
Ray Kurzweil},
 booktitle = {{IJCAI}},
 editor = {Sarit Kraus},
 pages = {5370--5378},
 title = {Improving Multilingual Sentence Embedding using Bi-directional Dual
Encoder with Additive Margin Softmax},
 year = {2019}
}

@InProceedings{yang2020towardsa,
  author    = {Yang, Jiacheng and Wang, Mingxuan and Zhou, Hao and Zhao, Chengqi and Yu, Yong and Zhang, Weinan and Li, Lei},
  booktitle = {Proc. of AAAI},
  title     = {Towards Making the Most of BERT in Neural Machine Translation},
  year      = {2020},
}

@inproceedings{yang2019xlnet,
 author = {Zhilin Yang and
Zihang Dai and
Yiming Yang and
Jaime G. Carbonell and
Ruslan Salakhutdinov and
Quoc V. Le},
 booktitle = {NeurIPS},
 editor = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
 pages = {5754--5764},
 title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
 year = {2019}
}

@Article{yi2019ectc,
  author  = {Yi, Cheng and Wang, Feng and Xu, Bo},
  journal = {Interspeech},
  title   = {ECTC-DOCD: An End-to-end Structure with CTC Encoder and OCD Decoder for Speech Recognition},
  year    = {2019},
  pages   = {4420--4424},
}

@inproceedings{yu2018multilingual,
 author = {Yu, Katherine  and
Li, Haoran  and
Oguz, Barlas},
 booktitle = {Proceedings of The Third Workshop on Representation Learning for {NLP}},
 pages = {175--179},
 title = {Multilingual Seq2seq Training with Similarity Loss for Cross-Lingual Document Classification},
 year = {2018}
}

@inproceedings{zhang2005decoding,
 author = {Zhang, Ruiqiang and Kikui, Genichiro and Yamamoto, Hirofumi and Lo, Wai-Kit},
 booktitle = {{IWSLT}},
 title = {A decoding algorithm for word lattice translation in speech translation},
 year = {2005}
}

@inproceedings{zhang2019lattice,
 author = {Zhang, Pei  and
Ge, Niyu  and
Chen, Boxing  and
Fan, Kai},
 booktitle = {Proc. of ACL},
 pages = {6475--6484},
 title = {Lattice Transformer for Speech Translation},
 year = {2019}
}

@inproceedings{zhang2020adaptive,
 author = {Zhang, Biao  and
Titov, Ivan  and
Haddow, Barry  and
Sennrich, Rico},
 booktitle = {Findings of EMNLP},
 pages = {2533--2544},
 title = {Adaptive Feature Selection for End-to-End Speech Translation},
 year = {2020}
}

@inproceedings{zhang2020improving,
 author = {Zhang, Biao  and
Williams, Philip  and
Titov, Ivan  and
Sennrich, Rico},
 booktitle = {Proc. of ACL},
 pages = {1628--1639},
 title = {Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation},
 year = {2020}
}

@InProceedings{zhou2018syllable,
  author    = {Zhou, Shiyu and Dong, Linhao and Xu, Shuang and Xu, Bo},
  booktitle = {Interspeech},
  title     = {Syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese},
  year      = {2018},
}

@inproceedings{zhu2020language,
 author = {Zhu, Changfeng  and
Yu, Heng  and
Cheng, Shanbo  and
Luo, Weihua},
 booktitle = {Proc. of ACL},
 pages = {1650--1655},
 title = {Language-aware Interlingua for Multilingual Neural Machine Translation},
 year = {2020}
}

@inproceedings{chen2020distilling,
 author = {Chen, Yen-Chun  and
Gan, Zhe  and
Cheng, Yu  and
Liu, Jingzhou  and
Liu, Jingjing},
 booktitle = {Proc. of ACL},
 pages = {7893--7905},
 title = {Distilling Knowledge Learned in {BERT} for Text Generation},
 year = {2020}
}

@inproceedings{chuang2020speechbert,
 author = {Chuang, Yung-Sung and Liu, Chi-Liang and Lee, Hung-Yi},
 booktitle = {INTERSPEECH},
 title = {{SpeechBERT}: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering},
 year = {2020}
}

@inproceedings{gu2018universal,
 author = {Gu, Jiatao  and
Hassan, Hany  and
Devlin, Jacob  and
Li, Victor O.K.},
 booktitle = {Proc. of NAACL-HLT},
 pages = {344--354},
 title = {Universal Neural Machine Translation for Extremely Low Resource Languages},
 year = {2018}
}

@InProceedings{he2019rethinking,
  author    = {Kaiming He and Ross B. Girshick and Piotr Doll{\'{a}}r},
  booktitle = {Proc. of ICCV},
  title     = {Rethinking ImageNet Pre-Training},
  year      = {2019},
  pages     = {4917--4926},
}

@inproceedings{huang2016attention,
 author = {Huang, Po-Yao  and
Liu, Frederick  and
Shiang, Sz-Rung  and
Oh, Jean  and
Dyer, Chris},
 booktitle = {Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
 pages = {639--645},
 title = {Attention-based Multimodal Neural Machine Translation},
 year = {2016}
}

@inproceedings{huang2019unicoder,
 author = {Huang, Haoyang  and
Liang, Yaobo  and
Duan, Nan  and
Gong, Ming  and
Shou, Linjun  and
Jiang, Daxin  and
Zhou, Ming},
 booktitle = {Proc. of EMNLP},
 pages = {2485--2494},
 title = {{U}nicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks},
 year = {2019}
}

@InProceedings{huang2021m3p,
  author    = {Huang, Haoyang and Su, Lin and Qi, Di and Duan, Nan and Cui, Edward and Bharti, Taroon and Zhang, Lei and Wang, Lijuan and Gao, Jianfeng and Liu, Bei and others},
  booktitle = {Proc. of CVPR},
  title     = {M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training},
  year      = {2021},
}

@Article{johnson2017googles,
  author  = {Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  journal = {TACL},
  title   = {{G}oogle{'}s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
  year    = {2017},
  pages   = {339--351},
  volume  = {5},
}

@inproceedings{lin2020pre,
 author = {Lin, Zehui  and
Pan, Xiao  and
Wang, Mingxuan  and
Qiu, Xipeng  and
Feng, Jiangtao  and
Zhou, Hao  and
Li, Lei},
 booktitle = {Proc. of EMNLP},
 pages = {2649--2663},
 title = {Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information},
 year = {2020}
}

@Article{liu2019roberta,
  author  = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke S. Zettlemoyer and Veselin Stoyanov},
  journal = {ArXiv},
  title   = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  year    = {2019},
  volume  = {abs/1907.11692},
}

@Article{liu2020multilingual,
  author  = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal = {TACL},
  title   = {Multilingual Denoising Pre-training for Neural Machine Translation},
  year    = {2020},
  pages   = {726--742},
  volume  = {8},
}

@article{mikolov2013exploiting,
 author = {Mikolov, Tomas and Le, Quoc V and Sutskever, Ilya},
 journal = {arXiv preprint arXiv:1309.4168},
 title = {Exploiting similarities among languages for machine translation},
 year = {2013}
}

@InProceedings{park2019specaugmenta,
  author    = {Daniel S. Park and William Chan and Yu Zhang and Chung-Cheng Chiu and Barret Zoph and Ekin Dogus Cubuk and Quoc V. Le},
  booktitle = {INTERSPEECH},
  title     = {SpecAugment: A Simple Augmentation Method for Automatic Speech Recognition},
  year      = {2019},
}

@InProceedings{pires2019how,
  author    = {Pires, Telmo and Schlinger, Eva and Garrette, Dan},
  booktitle = {Proc. of ACL},
  title     = {How Multilingual is Multilingual {BERT}?},
  year      = {2019},
  pages     = {4996--5001},
}

@InProceedings{long2021generative,
  author    = {Long, Quanyu and Wang, Mingxuan and Li, Lei},
  booktitle = {Proc. of NAACL-HLT},
  title     = {Generative Imagination Elevates Machine Translation},
  year      = {2021},
  pages     = {5738--5748},
}


@InProceedings{ramachandran2017unsupervised,
  author    = {Ramachandran, Prajit and Liu, Peter and Le, Quoc},
  booktitle = {Proc. of EMNLP},
  title     = {Unsupervised Pretraining for Sequence to Sequence Learning},
  year      = {2017},
  pages     = {383--391},
}

@InProceedings{song2019mass,
  author    = {Kaitao Song and Xu Tan and Tao Qin and Jianfeng Lu and Tie{-}Yan Liu},
  booktitle = {Proc. of ICML},
  title     = {{MASS:} Masked Sequence to Sequence Pre-training for Language Generation},
  year      = {2019},
  editor    = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  pages     = {5926--5936},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
}

@InProceedings{wang2019vatex,
  author    = {Xin Wang and Jiawei Wu and Junkun Chen and Lei Li and Yuan{-}Fang Wang and William Yang Wang},
  booktitle = {ICCV},
  title     = {VaTeX: {A} Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research},
  year      = {2019},
  pages     = {4580--4590},
}

@InProceedings{xie2020self,
  author    = {Qizhe Xie and Minh{-}Thang Luong and Eduard H. Hovy and Quoc V. Le},
  booktitle = {Proc. of CVPR},
  title     = {Self-Training With Noisy Student Improves ImageNet Classification},
  year      = {2020},
  pages     = {10684--10695},
}

@InProceedings{yang2020towards,
  author    = {Jiacheng Yang and Mingxuan Wang and Hao Zhou and Chengqi Zhao and Weinan Zhang and Yong Yu and Lei Li},
  booktitle = {Proc. of AAAI},
  title     = {Towards Making the Most of {BERT} in Neural Machine Translation},
  year      = {2020},
}

@inproceedings{zhu2020incorporating,
 author = {Jinhua Zhu and
Yingce Xia and
Lijun Wu and
Di He and
Tao Qin and
Wengang Zhou and
Houqiang Li and
Tie{-}Yan Liu},
 booktitle = {Proc. of ICLR},
 title = {Incorporating {BERT} into Neural Machine Translation},
 year = {2020}
}

@InProceedings{lin2021learning,
  author    = {Zehui Lin and Liwei Wu and Mingxuan Wang and Lei Li},
  booktitle = {Proc. of ACL},
  title     = {Learning Language Specific Sub-network for Multilingual Machine Translation},
  year      = {2021},
  month     = aug,
  abstract  = {Multilingual neural machine translation aimsat learning a single translation model for muliple  languages.  These  jointly  trained  mod-els often suffer from performance degradationon rich-resource language pairs. We attributethis degeneration to parameter interference. Inthis paper, we propose LaSS to jointly train asingle  unified  multilingual  MT  model.  LaSS learns Language Secific Sub-network (LaSS)for  each  language  pair  to  counter  parameterinterference.  Comprehensive  experiments  on IWSLT and WMT datasets with various Transformer  architectures  show  that  LaSS  obtainsgains on 36 language pairs by up to 1.2 BLEU.Besides, LaSS shows its strong generalization performance  at  easy  adaptation  to  new  lan-guage  pairs  and  zero-shot  translation.  LaSS boosts  zero-shot  translation  with  an  averageof  8.3  BLEU  on  30  language  pairs.},
  code      = {https://github.com/NLP-Playground/LaSS},
  eprint    = {https://arxiv.org/abs/2105.09259},
  timestamp = {2020-05-01},
}

@InProceedings{pan2021contrastive,
  author    = {Xiao Pan and Liwei Wu and Mingxuan Wang and Lei Li},
  booktitle = {Proc. of ACL},
  title     = {Contrastive Learning for Many-to-many Multilingual Neural Machine Translation},
  year      = {2021},
  month     = aug,
  code      = {https://github.com/PANXiao1994/mRASP2},
  eprint    = {https://arxiv.org/abs/2105.09501},
  timestamp = {2020-05-01},
}

@InProceedings{qian2021glancing,
  author    = {Lihua Qian and Hao Zhou and Yu Bao and Mingxuan Wang and Lin Qiu and Weinan Zhang and Yong Yu and Lei Li},
  booktitle = {Proc. of ACL},
  title     = {Glancing Transformer for Non-Autoregressive Neural Machine Translation},
  year      = {2021},
  month     = aug,
  eprint    = {https://arxiv.org/abs/2008.07905},
  timestamp = {2020-05-01},
}

@InProceedings{wang2021unire,
  author    = {Yijun Wang and Changzhi Sun and Yuanbin Wu and Hao Zhou and Lei Li and Junchi Yan},
  booktitle = {Proc. of ACL},
  title     = {UniRE: A Unified Label Space for Entity Relation Extraction},
  year      = {2021},
  month     = aug,
  timestamp = {2020-05-01},
}

@InProceedings{xu2021document,
  author    = {Runxin Xu and Tianyu Liu and Lei Li and Baobao Chang},
  booktitle = {Proc. of ACL},
  title     = {Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker},
  year      = {2021},
  month     = aug,
  code      = {https://github.com/RunxinXu/GIT},
  eprint    = {https://arxiv.org/abs/2105.14924},
  timestamp = {2020-05-01},
}

@InProceedings{xu2021vocabulary,
  author    = {Jingjing Xu and Hao Zhou and Chun Gan and Zaixiang Zheng and Lei Li},
  booktitle = {Proc. of ACL},
  title     = {Vocabulary Learning via Optimal Transport for Neural Machine Translation},
  year      = {2021},
  month     = aug,
  code      = {https://github.com/Jingjing-NLP/VOLT},
  eprint    = {https://arxiv.org/abs/2012.15671},
  timestamp = {2020-05-01},
}

@InProceedings{han2021learning,
  author    = {Chi Han and Mingxuan Wang and Heng Ji and Lei Li},
  booktitle = {Proc. of ACL-Findings},
  title     = {Learning Shared Semantic Space for Speech-to-Text Translation},
  year      = {2021},
  month     = aug,
  code      = {https://github.com/Glaciohound/Chimera-ST},
  eprint    = {https://arxiv.org/abs/2105.03095},
  timestamp = {2020-05-01},
}

@InProceedings{sun2021probabilistic,
  author    = {Changzhi Sun and Xinbo Zhang and Jiangjie Chen and Chun Gan and Yuanbin Wu and Jiaze Chen and Hao Zhou and Lei Li},
  booktitle = {Proc. of ACL-Findings},
  title     = {Probabilistic Graph Reasoning for Natural Proof Generation},
  year      = {2021},
  month     = aug,
  code      = {https://github.com/changzhisun/PRobr/},
  timestamp = {2020-05-01},
}

@InProceedings{wang2021contrastive,
  author    = {Danqing Wang and Jiaze Chen and Hao Zhou and Xipeng Qiu and Lei Li},
  booktitle = {Proc. of ACL-Findings},
  title     = {Contrastive Aligned Joint Learning for Multilingual Summarization},
  year      = {2021},
  month     = aug,
  timestamp = {2020-05-01},
}

@InProceedings{wu2021language,
  author    = {Liwei Wu and Shanbo Chen and Mingxuan Wang and Lei Li},
  booktitle = {Proc. of ACL-Findings},
  title     = {Language Tags Matter for Zero-Shot Neural Machine Translation},
  year      = {2021},
  month     = aug,
  timestamp = {2020-05-01},
}

@InProceedings{zhao2021neurst,
  author       = {Chengqi Zhao and Mingxuan Wang and Qianqian Dong and Rong Ye and Lei Li},
  booktitle    = {Proc. of ACL System Demonstrations},
  title        = {{NeurST}: Neural Speech Translation Toolkit},
  year         = {2021},
  month        = aug,
  code         = {https://github.com/bytedance/neurst},
  entrysubtype = {demo},
  eprint       = {https://arxiv.org/abs/2012.10018},
  timestamp    = {2020-05-01},
}

@InProceedings{jing2021adversarial,
  author    = {Mingxuan Jing and Wenbing Huang and Fuchun Sun and Xiaojian Ma and Tao Kong and Chuang Gan and Lei Li},
  booktitle = {Proc. of ICML},
  title     = {Adversarial Option-Aware Hierarchical Imitation Learning},
  year      = {2021},
  month     = jul,
}

@InProceedings{wang2021cross,
  author    = {Wang, Mingxuan and Bai, Hongxiao and Zhao, Hai and Li, Lei},
  booktitle = {Proc. of NAACL-HLT: Industry Papers},
  title     = {Cross-lingual Supervision Improves Unsupervised Neural Machine Translation},
  year      = {2021},
  address   = {Online},
  month     = jun,
  pages     = {89--96},
  publisher = {Association for Computational Linguistics},
  abstract  = {We propose to improve unsupervised neural machine translation with cross-lingual supervision (), which utilizes supervision signals from high resource language pairs to improve the translation of zero-source languages. Specifically, for training En-Ro system without parallel corpus, we can leverage the corpus from En-Fr and En-De to collectively train the translation from one language into many languages under one model. {\%} is based on multilingual models which require no changes to the standard unsupervised NMT. Simple and effective, significantly improves the translation quality with a big margin in the benchmark unsupervised translation tasks, and even achieves comparable performance to supervised NMT. In particular, on WMT{'}14 -tasks achieves 37.6 and 35.18 BLEU score, which is very close to the large scale supervised setting and on WMT{'}16 -tasks achieves 35.09 BLEU score which is even better than the supervised Transformer baseline.},
  eprint    = {https://arxiv.org/abs/2004.03137},
  owner     = {lilei.02},
}

@InProceedings{wang2021lightseq,
  author    = {Wang, Xiaohui and Xiong, Ying and Wei, Yang and Wang, Mingxuan and Li, Lei},
  booktitle = {Proc. of NAACL-HLT: Industry Papers},
  title     = {{L}ight{S}eq: A High Performance Inference Library for Transformers},
  year      = {2021},
  address   = {Online},
  month     = jun,
  pages     = {113--120},
  publisher = {Association for Computational Linguistics},
  abstract  = {Transformer and its variants have achieved great success in natural language processing. Since Transformer models are huge in size, serving these models is a challenge for real industrial applications. In this paper, we propose , a highly efficient inference library for models in the Transformer family. includes a series of GPU optimization techniques to both streamline the computation of Transformer layers and reduce memory footprint. supports models trained using PyTorch and Tensorflow. Experimental results on standard machine translation benchmarks show that achieves up to 14x speedup compared with TensorFlow and 1.4x speedup compared with , a concurrent CUDA implementation. The code will be released publicly after the review.},
  code      = {https://github.com/bytedance/lightseq},
  eprint    = {https://arxiv.org/abs/2010.13887},
  owner     = {lilei.02},
}

@InProceedings{liang2021finding,
  author    = {Jianze Liang and Chengqi Zhao and Mingxuan Wang and Xipeng Qiu and Lei Li},
  booktitle = {Proc. of AAAI},
  title     = {Finding Sparse Structure for Domain Specific Neural Machine Translation},
  year      = {2021},
  month     = feb,
  code      = {https://github.com/ohlionel/Prune-Tune},
  eprint    = {https://arxiv.org/abs/2012.10586},
  owner     = {lilei.02},
  thumbnail = {liang2021finding_prune_tune_diagram.svg},
  url       = {https://ohlionel.github.io/project/Prune-Tune/},
}

@InProceedings{wu2020volctrans,
  author       = {Liwei Wu and Xiao Pan and Zehui Lin and Yaoming Zhu and Mingxuan Wang and Lei Li},
  booktitle    = {Proceedings of the Fifth Conference on Machine Translation (Volume 2: Shared Task Papers)},
  title        = {The Volctrans Machine Translation System for WMT20},
  year         = {2020},
  month        = nov,
  comment      = {Winner in WMT20 Machine Translation Contest of Chinese-English, German-English, French-German,  English-Khmer, and English-Pashto languages.},
  entrysubtype = {workshop},
  eprint       = {https://arxiv.org/abs/2010.14806},
  owner        = {lilei.02},
  thumbnail    = {volctrans-logo-eng.png},
}

@InProceedings{xu2020volctrans,
  author       = {Runxin Xu and Zhuo Zhi and Jun Cao and Mingxuan Wang and Lei Li},
  booktitle    = {Proceedings of the Fifth Conference on Machine Translation (Volume 2: Shared Task Papers)},
  title        = {Volctrans Parallel Corpus Filtering System for WMT 2020},
  year         = {2020},
  month        = nov,
  comment      = {Winner of WMT20 Parallel Corpus Filtering tasks on Pashto and Khmer languages.},
  entrysubtype = {workshop},
  eprint       = {https://arxiv.org/abs/2010.14029},
  owner        = {lilei.02},
}

@InProceedings{li2020sentence,
  author    = {Bohan Li and Hao Zhou and Junxian He and Mingxuan Wang and Yiming Yang and Lei Li},
  booktitle = {Proc. of EMNLP},
  title     = {On the Sentence Embeddings from Pre-trained Language Models},
  year      = {2020},
  month     = nov,
  abstract  = {Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks.},
  code      = {https://github.com/bohanli/BERT-flow},
  eprint    = {https://arxiv.org/abs/2011.05864},
  owner     = {lilei.02},
}

@InProceedings{zeng2020double,
  author    = {Shuang Zeng and Runxin Xu and Baobao Chang and Lei Li},
  booktitle = {Proc. of EMNLP},
  title     = {Double Graph Based Reasoning for Document-level Relation Extraction},
  year      = {2020},
  month     = nov,
  abstract  = {Document-level relation extraction aims to extract relations among entities within a docuent. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention- level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art.},
  code      = {https://github.com/PKUnlp-icler/GAIN},
  eprint    = {https://arxiv.org/abs/2009.13752},
  owner     = {lilei.02},
}

@InProceedings{ru2020active,
  author    = {Dongyu Ru and Jiangtao Feng and Lin Qiu and Hao Zhou and Mngxuan Wang and Weinan Zhang and Yong Yu and Lei Li},
  booktitle = {Proc. of EMNLP- Findings},
  title     = {Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete Space},
  year      = {2020},
  month     = nov,
  abstract  = {Active learning for sentence understanding aims at discovering informative unlabeled data for annotation and therefore reducing the demand for labeled data. We argue that the typical uncertainty sampling method for active learning is time-consuming and can hardly work in real-time, which may lead to ineffective sample selection. We propose adversarial uncertainty sampling in discrete space (AUSDS) to retrieve informative unlabeled samples more efficiently. AUSDS maps sentences into latent space generated by the popuar pre-trained language models, and discover informative unlabeled text samples for annotation via adversarial attack. The proposed approach is extremely efficient compared with traditional uncertainty sampling with more than 10x speedup. Experimental results on five datasets show that AUSDS outperforms strong baselines on effectiveness.},
  eprint    = {https://arxiv.org/abs/2004.08046},
}

@InProceedings{zhang2020language,
  author    = {Maosen Zhang and Nan Jiang and Lei Li and Yexiang Xue},
  booktitle = {Proc. of EMNLP - Findings},
  title     = {Language Generation via Combinatorial Constraint Satisfaction: A Tree Search Enhanced {Monte}-{Carlo} Approach},
  year      = {2020},
  month     = nov,
  abstract  = {Generating natural language under complex constraints is a principled formulation towards controllable text generation. We present a framework to allow specification of combinatorial constraints for sentence generation. We propose TSMH1, an efficient method to generate high likelihood sentences with respect to a pre-trained language model while satisfying the constraints. Our approach is highly flexible, requires no task-specific training, and leverages efficient constraint satisfaction solving techniques. To better handle the combinatorial constraints, a tree search algorithm is embedded into the proposal process of the Markov chain Monte Carlo (MCMC) to explore candidates that satisfy more constraints. Compared to existing MCMC approaches, our sampling approach has a better mixing performance. Experiments show that TSMH achieves consistent and significant improvement on multiple language generation tasks.},
  code      = {https://github.com/Milozms/TSMH},
  eprint    = {https://arxiv.org/abs/2011.12334},
  owner     = {lilei.02},
}

@InProceedings{song2020improving,
  author    = {Yuxuan Song and Ning Miao and Hao Zhou and Lantao Yu and Mingxuan Wang and Lei Li},
  booktitle = {Proc. of AISTATS},
  title     = {Improving Maximum Likelihood Training for Text Generation with Density Ratio Estimation},
  year      = {2020},
  month     = aug,
  abstract  = {Auto-regressive  sequence  generative  models trained by Maximum Likelihood Estimation suffer  the  exposure  bias  problem  in  practical finite sample scenarios.  The crux is that the number of training samples for Maximum Likelihood Estimation is usually limited and the input data distributions are different at training and inference stages.  Many method shave been proposed to solve the above problem (Yu et al., 2017; Lu et al., 2018), which relies  on  sampling  from  the  non-stationary model distribution and suffers from high variance  or  biased  estimations.   In  this  paper, we  proposeÏˆ-MLE,  a  new  training  scheme for auto-regressive sequence generative models, which is effective and stable when operating at large sample space encountered in text generation.   We  derive  our  algorithm  from a  new  perspective  of  self-augmentation  and introduce  bias  correction  with  density  ratio estimation.   Extensive  experimental  results on  synthetic  data  and  real-world  text  generation  tasks  demonstrate  that  our  method stably outperforms Maximum Likelihood Estimation and other state-of-the-art sequence generative  models  in  terms  of  both  quality and diversity.},
  eprint    = {https://arxiv.org/abs/2007.06018},
  timestamp = {2020-01-07},
}

@InProceedings{shi2020dispersed,
  author    = {Wenxian Shi and Hao Zhou and Ning Miao and Lei Li},
  booktitle = {Proc. of ICML},
  title     = {Dispersed Exponential Family Mixture {VAE}s for Interpretable Text Generation},
  year      = {2020},
  month     = jul,
  abstract  = {Deep generative models are commonly used for generating images and text. Interpretability of these models is one important pursuit, other than the generation quality. Variational auto-encoder (VAE) with Gaussian distribution as prior has been successfully applied in text generation, but it is hard to interpret the meaning of the latent variable. To enhance the controllability and interpretability, one can replace the Gaussian prior with a mixture of Gaussian distributions (GM-VAE), whose mixture components could be related to hidden semantic aspects of data. In this paper, we generalize the practice and introduce DEM-VAE, a class of models for text generation using VAEs with a mixture distribution of exponential family. Unfortunately, a standard variational training algorithm fails due to the mode-collapse problem. We theoretically identify the root cause of the problem and propose an effective algorithm to train DEM-VAE. Our method penalizes the training with an extra dispersion term to induce a well-structured latent space. Experimental results show that our approach does obtain a meaningful space, and it outperforms strong baselines in text generation benchmarks. The code is available at https://github.com/wenxianxian/demvae.},
  code      = {https://github.com/wenxianxian/demvae},
  eprint    = {https://arxiv.org/abs/1906.06719},
  thumbnail = {shi2020dispersed_dgmvae_latent.png},
  video     = {https://slideslive.com/38928051},
}

@InProceedings{miao2020do,
  author    = {Ning Miao and Yuxuan Song and Hao Zhou and Lei Li},
  booktitle = {Proc. of ACL - short papers},
  title     = {Do you have the right scissors? Tailoring Pre-trained Language Models via {Monte}-{Carlo} Methods},
  year      = {2020},
  month     = jul,
  abstract  = {It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data.  In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to  over- and/or under-estimation problem.  In this paper, we propose MC-Taylor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones. Experiments on a variety of text generation datasets show that MC-Taylor consistently and significantly outperforms the fine-tuning approach. Our code is available at \url{https://github.com/NingMiao/MC-tailor}.},
  code      = {https://github.com/NingMiao/MC-tailor},
  eprint    = {https://arxiv.org/abs/2007.06162},
  timestamp = {2020-05-01},
  video     = {http://slideslive.com/38928919},
}

@InProceedings{xu2020xiaomingbot,
  author       = {Runxin Xu and Jun Cao and Mingxuan Wang and Jiaze Chen and Hao Zhou and Ying Zeng and Yuping Wang and Li Chen and Xiang Yin and Xijin Zhang and Songcheng Jiang and Yuxuan Wang and Lei Li},
  booktitle    = {Proc. of ACL: System Demonstrations},
  title        = {Xiaomingbot: A Multilingual Robot News Reporter},
  year         = {2020},
  month        = jul,
  abstract     = {This paper proposes the building of Xiaomingbot, an intelligent, multilingual and multi-modal software robot equipped with four integral capabilities: news generation, news translation, news reading and avatar animation. Its system summarizes Chinese news that it automatically generates from data tables. Next, it translates the summary or the full article into multiple languages, and reads the multilingual rendition through synthesized speech. Notably, Xiaomingbot utilizes a voice cloning technology to synthesize the speech trained from a real personâ€™s voice data in one input language. The proposed system enjoys several merits: it has an animated avatar, and is able to generate and read multilingual news. Since it was put into practice, Xiaomingbot has written over 600,000 articles, and gained over 150,000 followers on social media platforms.},
  entrysubtype = {demo},
  timestamp    = {2020-05-01},
  url          = {https://xiaomingbot.github.io},
}

@InProceedings{ye2020variational,
  author    = {Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},
  booktitle = {Proc. of ICLR},
  title     = {Variational Template Machine for Data-to-Text Generation},
  year      = {2020},
  month     = apr,
  abstract  = {How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable "templates" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able generate more diversely while keeping a good fluency and quality.},
  code      = {https://github.com/ReneeYe/VariationalTemplateMachine},
  eprint    = {https://openreview.net/forum?id=HkejNgBtPB},
  video     = {https://iclr.cc/virtual_2020/poster_HkejNgBtPB.html},
}

@InProceedings{zheng2020mirror,
  author    = {Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xinyu Dai and Jiajun Chen},
  booktitle = {Proc. of ICLR},
  title     = {Mirror Generative Models for Neural Machine Translation},
  year      = {2020},
  month     = apr,
  abstract  = {Training neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in all a variety of scenarios and language pairs, including resource-rich and low-resource languages.},
  addendum  = {Oral, 1.9\% acceptance rate},
  eprint    = {https://openreview.net/forum?id=HkxQRTNYPH},
  owner     = {lilei.02},
  video     = {https://iclr.cc/virtual_2020/poster_HkxQRTNYPH.html},
}

@InProceedings{wu2020importance,
  author    = {Qingyang Wu and Lei Li and Hao Zhou and Ying Zeng and Zhou Yu},
  booktitle = {Proc. of AAAI},
  title     = {Importance-Aware Learning for Neural Headline Editing},
  year      = {2020},
  month     = feb,
  abstract  = {Many social media news writers are not professionally trained. Therefore, social media platforms have to hire professional editors to adjust amateur headlines to attract more readers. We propose to automate this headline editing process through neural network models to provide more immediate writing support for these social media news writers. To train such a neural headline editing model, we collected a dataset which contains articles with original headlines and professionally edited headlines. However, it is expensive to collect a large number of professionally edited headlines. To solve this low-resource problem, we design an encoder-decoder model which leverages large scale pre-trained language models. We further improve the pre-trained model's quality by introducing a headline generation task as an intermediate task before the headline editing task. Also, we propose Self Importance-Aware (SIA) loss to address the different levels of editing in the dataset by down-weighting the importance of easily classified tokens and sentences. With the help of Pre-training, Adaptation, and SIA, the model learns to generate headlines in the professional editor's style. Experimental results show that our method significantly improves the quality of headline editing comparing against previous methods.},
  eprint    = {https://arxiv.org/abs/1912.01114},
}

@InProceedings{miao2019kernelized,
  author    = {Miao, Ning and Zhou, Hao and Zhao, Chengqi and Shi, Wenxian and Li, Lei},
  booktitle = {NeurIPS},
  title     = {Kernelized {Bayesian} Softmax for Text Generation},
  year      = {2019},
  month     = dec,
  abstract  = {Neural models for text generation require a softmax layer with proper word embeddings during the decoding phase. Most existing approaches adopt single point embedding for each word. However, a word may have multiple senses according to different context, some of which might be distinct. In this paper, we propose KerBS, a novel approach for learning better embeddings for text generation. KerBS embodies two advantages: a) it employs a Bayesian composition of embeddings for words with multiple senses; b) it is adaptive to semantic variances of words and
robust to rare sentence context by imposing learned kernels to capture the closeness of words (senses) in the embedding space. Empirical studies show that KerBS significantly boosts the performance of several text generation tasks.},
  code      = {https://github.com/NingMiao/KerBS},
  eprint    = {https://arxiv.org/abs/1911.00274},
}

@InProceedings{wang2019towards,
  author    = {Wang, Mingxuan and Xie, Jun and Tan, Zhixing and Su, Jinsong and Xiong, Deyi and Li, Lei},
  booktitle = {Proc. of EMNLP},
  title     = {Towards Linear Time Neural Machine Translation with {Capsule} Networks},
  year      = {2019},
  month     = nov,
  abstract  = {In this study, we first investigate a novel capsule network with dynamic routing for linear time Neural Machine Translation (NMT), referred as CAPSNMT. CAPSNMT uses an aggregation mechanism to map the source sentence into a matrix with pre-determined size, and then applys a deep LSTM network to decode the target sequence from the source representation. Unlike the previous work (Sutskever et al., 2014) to store the source sentence with a passive and bottom-up way, the dynamic routing policy encodes the source sentence with an iterative process to decide the credit attribution between nodes from lower and higher layers. CAPSNMT has two core properties: it runs in time that is linear in the length of the sequences and provides a more flexible way to aggregate the part-whole information of the source sentence. On WMT14 English-German task and a larger WMT14 English-French task, CAPSNMT achieves comparable results with the Transformer system. We also devise new hybrid architectures intended to combine the strength of CAPSNMT and the RNMT model. Our hybrid models obtain state-of-the-arts results on both benchmark datasets. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for sequence to sequence problems},
  eprint    = {https://arxiv.org/abs/1811.00287},
}

@InProceedings{weng2019correct,
  author    = {Weng, Rongxiang and Zhou, Hao and Huang, Shujian and Xia, Yifan and Li, Lei and Chen, Jiajun},
  booktitle = {Proc. of IJCAI},
  title     = {{Correct-and-Memorize}: Learning to Translate from Interactive Revisions},
  year      = {2019},
  month     = aug,
  abstract  = {State-of-the-art machine translation models are stillnot on a par with human translators. Previous worktakes human interactions into the neural machine translation process to obtain improved results in target languages. However, not all model-translation errors are equal some are critical while others are minor. In the mean while, same translation mistakes occur repeatedly in similar context. To solve bothissues, we propose CAMIT, a novel method for translating in an interactive environment. Our proposed method works with critical revision instructions,therefore allows human to correct arbitrary words in model-translated sentences. In addition,CAMIT learns from and softly memorizes revision actions based on the context, alleviating the issue of repeating mistakes. Experiments in both ideal and real interactive translation settings demonstrate that our proposed CAMIT enhances machine translation results significantly while requires fewer revision instructions from human compared to previous methods.},
  addendum  = {Oral},
}

@InProceedings{bao2019generating,
  author    = {Bao, Yu and Zhou, Hao and Huang, Shujian and Li, Lei and Mou, Lili and Vechtomova, Olga and Dai, Xinyu and Chen, Jiajun},
  booktitle = {Proc. of ACL},
  title     = {Generating Sentences from Disentangled Syntactic and Semantic Spaces},
  year      = {2019},
  month     = jul,
  abstract  = {Variational auto-encoders (VAEs) are widely used in natural language generation due to the regularization of the latent space. However, generating sentences from the continuous latent space does not explicitly model the syntactic information. In this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. Our proposed method explicitly models syntactic information in the VAEâ€™s latent space by using the linearized tree sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntax-transfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work.},
  code      = {https://github.com/baoy-nlp/DSS-VAE},
}

@InProceedings{qiu2019dynamically,
  author    = {Qiu, Lin and Xiao, Yunxuan and Qu, Yanru and Zhou, Hao and Li, Lei and Zhang, Weinan and Yu, Yong},
  booktitle = {Proc. of ACL},
  title     = {Dynamically Fused Graph Network for Multi-hop Reasoning},
  year      = {2019},
  month     = jul,
  abstract  = {Text-based question answering (TBQA) has been studied extensively in recent years. Most existing approaches focus on finding the answer to a question within a single paragraph. However, many difficult questions require multiple supporting evidence from scattered text across two or more documents. In this paper, we propose the Dynamically Fused Graph Network (DFGN), a novel method to answer those questions requiring multiple scattered evidence and reasoning over them. Inspired by humanâ€™s step-by-step reasoning behavior, DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query, explores along the entity graph dynamically built from the text, and gradually finds relevant supporting entities from the given documents. We evaluate DFGN on HotpotQA, a public TBQA dataset requiring multi-hop reasoning. DFGN achieves competitive results on the public board. Furthermore, our analy- sis shows DFGN could produce interpretable reasoning chains.},
  file      = {:pubs/qiu-acl2019-dfgn.pdf:PDF},
}

@InProceedings{zhang2019generating,
  author    = {Zhang, Huangzhao and Miao, Ning and Zhou, Hao and Li, Lei},
  booktitle = {Proc. of ACL - short papers},
  title     = {Generating Fluent Adversarial Examples for Natural Languages},
  year      = {2019},
  month     = jul,
  abstract  = {Efficiently building an adversarial attacker fornatural language processing (NLP) tasks is areal challenge. Firstly, as the sentence spaceis discrete, it is difficult to make small perturbations along the direction of gradients. Secondly,the fluency of the generated examples can not be guaranteed. In this paper, we propose MHA, which addresses both problemsby performing Metropolis-Hastings sampling,whose proposal is designed with the guidanceof gradients. Experiments on IMDB and SNLIshow that our proposed MHA outperforms thebaseline model on attacking capability. Adversarial training with MHA also leads to better robustness and performance.},
  file      = {:pubs/zhang-acl2019generation-cgmh-adversarial.pdf:PDF},
}

@InProceedings{wu2019unified,
  author    = {Wu, Hao and Mao, Jiayuan and Zhang, Yufeng and Sun, Weiwei and Jiang, Yuning and Li, Lei and Ma, Wei-Ying},
  booktitle = {Proc. of CVPR},
  title     = {Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations},
  year      = {2019},
  month     = jun,
  abstract  = {We propose Unified Visual-Semantic Embeddings (VSE)
for learning a joint space for scene representation and textual
semantics. It unifies the embeddings of concepts at different
levels: objects, attributes, relations and full scenes. We
view the sentential semantics as a combination of different
semantic components such as object or relational descriptors,
and align their embeddings with different regions of a
scene. A contrastive learning approach is proposed for the
effective learning of such fine-grained alignment from only
image-caption pairs. We also present a simple yet effective
approach that enforces the coverage of caption embeddings
on the semantic components that appear in the sentence. We
demonstrate that the Unified VSE outperforms other baselines
on cross-modal retrieval tasks and the enforcement
of the semantic coverage improves modelsâ€™ robustness in
defending text-domain adversarial attacks. Moreover, such
robustness empowers the use of visual cues to accurately
resolve word dependencies in novel sentences.},
  addendum  = {Oral},
  file      = {:pubs/wu-cvpr2019-vse.pdf:PDF},
}

@InProceedings{miao2019cgmh,
  author    = {Miao, Ning and Zhou, Hao and Mou, Lili and Yan, Rui and Li, Lei},
  booktitle = {Proc. of AAAI},
  title     = {{CGMH}: Constrained Sentence Generation by Metropolis-Hastings Sampling},
  year      = {2019},
  month     = jan,
  abstract  = {In real-world applications of natural language generation,
there are often constraints on the target sentences in addition
to fluency and naturalness requirements. Existing language
generation techniques are usually based on recurrent
neural networks (RNNs). However, it is non-trivial to impose
constraints on RNNs while maintaining generation quality,
since RNNs generate sentences sequentially (or with beam
search) from the first word to the last. In this paper, we propose
CGMH, a novel approach using Metropolis-Hastings
sampling for constrained sentence generation. CGMH allows
complicated constraints such as the occurrence of multiple
keywords in the target sentences, which cannot be handled in
traditional RNN-based approaches. Moreover, CGMH works
in the inference stage, and does not require parallel corpora
for training.We evaluate our method on a variety of tasks, including
keywords-to-sentence generation, unsupervised sentence
paraphrasing, and unsupervised sentence error correction.
CGMH achieves high performance compared with previous
supervised methods for sentence generation. Our code
is released at https://github.com/NingMiao/CGMH},
  addendum  = {Oral},
  code      = {https://github.com/NingMiao/CGMH},
  eprint    = {http://arxiv.org/abs/1811.10996},
  slides    = {pubs/miao2019cgmh-ppt.pdf},
}

@InProceedings{shi2018tree,
  author    = {Shi, Haoyue and Zhou, Hao and Chen, Jiaze and Li, Lei},
  booktitle = {Proc. of EMNLP},
  title     = {On Tree-Based Neural Sentence Modeling},
  year      = {2018},
  month     = oct,
  abstract  = {Neural networks with tree-based sentence encoders have shown better results on many downstream tasks. Most of existing tree-based encoders adopt syntactic parsing trees as the explicit structure prior. To study the effectiveness of different tree structures, we replace the parsing trees with trivial trees (i.e., binary balanced tree, left-branching tree and right-branching tree) in the encoders. Though trivial trees contain no syntactic information, those encoders get competitive or even better results on all of the ten downstream tasks we investigated. This surprising result indicates that explicit syntax guidance may not be the main contributor to the superior performances of tree-based neural sentence modeling. Further analysis show that tree modeling gives better results when crucial words are closer to the final representation. Additional experiments give more clues on how to design an effective tree-based encoder.},
  code      = {https://github.com/ExplorerFreda/TreeEnc},
  eprint    = {https://arxiv.org/abs/1808.09644},
  owner     = {lilei.02},
}

@InProceedings{kingma2015adam,
  author    = {Diederik P. Kingma and Jimmy Ba},
  booktitle = {Proc. of ICLR},
  title     = {Adam: {A} Method for Stochastic Optimization},
  year      = {2015},
  editor    = {Yoshua Bengio and Yann LeCun},
}

@inproceedings{aharoni2019massively,
 author = {Aharoni, Roee  and
Johnson, Melvin  and
Firat, Orhan},
 booktitle = {Proc. of NAACL-HLT},
 pages = {3874--3884},
 title = {Massively Multilingual Neural Machine Translation},
 year = {2019}
}

@InProceedings{alshedivat2019consistency,
  author    = {Al-Shedivat, Maruan and Parikh, Ankur},
  booktitle = {Proc. of NAACL-HLT},
  title     = {Consistency by Agreement in Zero-Shot Neural Machine Translation},
  year      = {2019},
  pages     = {1184--1197},
}

@article{arivazhagan2019massively,
 archiveprefix = {arXiv},
 author = {Naveen Arivazhagan and
Ankur Bapna and
Orhan Firat and
Dmitry Lepikhin and
Melvin Johnson and
Maxim Krikun and
Mia Xu Chen and
Yuan Cao and
George F. Foster and
Colin Cherry and
Wolfgang Macherey and
Zhifeng Chen and
Yonghui Wu},
 eprint = {1907.05019},
 journal = {CoRR},
 title = {Massively Multilingual Neural Machine Translation in the Wild: Findings
and Challenges},
 volume = {abs/1907.05019},
 year = {2019}
}

@InProceedings{bahdanau2015neural,
  author    = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  booktitle = {Proc. of ICLR},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  year      = {2015},
  editor    = {Yoshua Bengio and Yann LeCun},
}

@inproceedings{bapna2019simple,
 author = {Bapna, Ankur  and
Firat, Orhan},
 booktitle = {Proc. of EMNLP},
 pages = {1538--1548},
 title = {Simple, Scalable Adaptation for Neural Machine Translation},
 year = {2019}
}

@InProceedings{sennrich2016neural,
  author    = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle = {Proc. of ACL},
  title     = {Neural Machine Translation of Rare Words with Subword Units},
  year      = {2016},
  pages     = {1715--1725},
}

@inproceedings{chen2017teacher,
 author = {Chen, Yun  and
Liu, Yang  and
Cheng, Yong  and
Li, Victor O.K.},
 booktitle = {Proc. of ACL},
 pages = {1925--1935},
 title = {A Teacher-Student Framework for Zero-Resource Neural Machine Translation},
 year = {2017}
}

@inproceedings{chen2018zero,
 author = {Yun Chen and
Yang Liu and
Victor O. K. Li},
 booktitle = {Proc. of AAAI},
 editor = {Sheila A. McIlraith and
Kilian Q. Weinberger},
 pages = {5086--5093},
 title = {Zero-Resource Neural Machine Translation with Multi-Agent Communication
Game},
 year = {2018}
}

@InProceedings{chen2020simple,
  author    = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey E. Hinton},
  booktitle = {Proc. of ICML},
  title     = {A Simple Framework for Contrastive Learning of Visual Representations},
  year      = {2020},
  pages     = {1597--1607},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
}

@InProceedings{choi2018improving,
  author    = {Choi, Gyu-Hyeon and Shin, Jong-Hun and Kim, Young-Kil},
  booktitle = {LREC},
  title     = {Improving a Multi-Source Neural Machine Translation Model with Corpus Extension for Low-Resource Languages},
  year      = {2018},
}

@Article{chu2019multilingual,
  author        = {Chenhui Chu and Raj Dabre},
  journal       = {CoRR},
  title         = {Multilingual Multi-Domain Adaptation Approaches for Neural Machine Translation},
  year          = {2019},
  volume        = {abs/1906.07978},
  archiveprefix = {arXiv},
  eprint        = {1906.07978},
}

@InProceedings{tran2020cross,
  author    = {Chau Tran and Yuqing Tang and Xian Li and Jiatao Gu},
  booktitle = {NeurIPS},
  title     = {Cross-lingual Retrieval for Iterative Self-Supervised Training},
  year      = {2020},
  editor    = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien Lin},
}

@inproceedings{currey2019zero,
 author = {Currey, Anna  and
Heafield, Kenneth},
 booktitle = {Proceedings of the 3rd Workshop on Neural Generation and Translation},
 pages = {99--107},
 title = {Zero-Resource Neural Machine Translation with Monolingual Pivot Data},
 year = {2019}
}

@article{dabre2017enabling,
 archiveprefix = {arXiv},
 author = {Raj Dabre and
Fabien Cromier{\`{e}}s and
Sadao Kurohashi},
 eprint = {1702.06135},
 journal = {CoRR},
 title = {Enabling Multi-Source Neural Machine Translation By Concatenating
Source Sentences In Multiple Languages},
 volume = {abs/1702.06135},
 year = {2017}
}

@InProceedings{post2018call,
  author    = {Post, Matt},
  booktitle = {Proceedings of the Third Conference on Machine Translation: Research Papers},
  title     = {A Call for Clarity in Reporting {BLEU} Scores},
  year      = {2018},
  pages     = {186--191},
}

@InProceedings{wang2019learning,
  author    = {Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F. and Chao, Lidia S.},
  booktitle = {Proc. of ACL},
  title     = {Learning Deep Transformer Models for Machine Translation},
  year      = {2019},
  pages     = {1810--1822},
}

@inproceedings{dong2015multi,
 author = {Dong, Daxiang  and
Wu, Hua  and
He, Wei  and
Yu, Dianhai  and
Wang, Haifeng},
 booktitle = {Proc. of ACL},
 pages = {1723--1732},
 title = {Multi-Task Learning for Multiple Language Translation},
 year = {2015}
}

@article{escolano2020training,
 archiveprefix = {arXiv},
 author = {Carlos Escolano and
Marta R. Costa{-}juss{\`{a}} and
Jos{\'{e}} A. R. Fonollosa and
Mikel Artetxe},
 eprint = {2006.01594},
 journal = {CoRR},
 title = {Training Multilingual Machine Translation by Alternately Freezing
Language-Specific Encoders-Decoders},
 volume = {abs/2006.01594},
 year = {2020}
}

@Article{fan2020english,
  author        = {Angela Fan and Shruti Bhosale and Holger Schwenk and Zhiyi Ma and Ahmed El{-}Kishky and Siddharth Goyal and Mandeep Baines and Onur Celebi and Guillaume Wenzek and Vishrav Chaudhary and Naman Goyal and Tom Birch and Vitaliy Liptchinsky and Sergey Edunov and Edouard Grave and Michael Auli and Armand Joulin},
  journal       = {CoRR},
  title         = {Beyond English-Centric Multilingual Machine Translation},
  year          = {2020},
  volume        = {abs/2010.11125},
  archiveprefix = {arXiv},
  eprint        = {2010.11125},
}

@article{fang2020cert,
 archiveprefix = {arXiv},
 author = {Hongchao Fang and
Pengtao Xie},
 eprint = {2005.12766},
 journal = {CoRR},
 title = {{CERT:} Contrastive Self-supervised Learning for Language Understanding},
 volume = {abs/2005.12766},
 year = {2020}
}

@inproceedings{gu2019improved,
 author = {Gu, Jiatao  and
Wang, Yong  and
Cho, Kyunghyun  and
Li, Victor O.K.},
 booktitle = {Proc. of ACL},
 pages = {1258--1268},
 title = {Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations},
 year = {2019}
}

@InProceedings{ha2017effective,
  author        = {Thanh{-}Le Ha and Jan Niehues and Alexander H. Waibel},
  booktitle     = {ISWLT},
  title         = {Effective Strategies in Zero-Shot Neural Machine Translation},
  year          = {2017},
  volume        = {abs/1711.07893},
  archiveprefix = {arXiv},
  eprint        = {1711.07893},
}

@InProceedings{he2020momentum,
  author    = {Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross B. Girshick},
  booktitle = {Proc. of CVPR},
  title     = {Momentum Contrast for Unsupervised Visual Representation Learning},
  year      = {2020},
  pages     = {9726--9735},
}

@InProceedings{ji2020cross,
  author    = {Baijun Ji and Zhirui Zhang and Xiangyu Duan and Min Zhang and Boxing Chen and Weihua Luo},
  booktitle = {Proc. of AAAI},
  title     = {Cross-Lingual Pre-Training Based Transfer for Zero-Shot Neural Machine Translation},
  year      = {2020},
  pages     = {115--122},
}

@inproceedings{kim2019effective,
 author = {Kim, Yunsu  and
Gao, Yingbo  and
Ney, Hermann},
 booktitle = {Proc. of ACL},
 pages = {1246--1257},
 title = {Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies},
 year = {2019}
}

@InProceedings{lample2018unsupervised,
  author    = {Guillaume Lample and Alexis Conneau and Ludovic Denoyer and Marc'Aurelio Ranzato},
  booktitle = {Proc. of ICLR},
  title     = {Unsupervised Machine Translation Using Monolingual Corpora Only},
  year      = {2018},
}

@InProceedings{misra2020self,
  author    = {Ishan Misra and Laurens van der Maaten},
  booktitle = {Proc. of CVPR},
  title     = {Self-Supervised Learning of Pretext-Invariant Representations},
  year      = {2020},
  pages     = {6706--6716},
}

@inproceedings{siddhant2020leveraging,
 author = {Siddhant, Aditya  and
Bapna, Ankur  and
Cao, Yuan  and
Firat, Orhan  and
Chen, Mia  and
Kudugunta, Sneha  and
Arivazhagan, Naveen  and
Wu, Yonghui},
 booktitle = {Proc. of ACL},
 pages = {2827--2835},
 title = {Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation},
 year = {2020}
}

@inproceedings{tan2019multilingual,
 author = {Xu Tan and
Yi Ren and
Di He and
Tao Qin and
Zhou Zhao and
Tie{-}Yan Liu},
 booktitle = {Proc. of ICLR},
 title = {Multilingual Neural Machine Translation with Knowledge Distillation},
 year = {2019}
}

@Article{artetxe2019massively,
  author  = {Artetxe, Mikel and Schwenk, Holger},
  journal = {TACL},
  title   = {Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond},
  year    = {2019},
  pages   = {597--610},
  volume  = {7},
}

@InProceedings{tian2020contrastive,
  author    = {Yonglong Tian and Dilip Krishnan and Phillip Isola},
  booktitle = {Proc. of ECCV},
  title     = {Contrastive Multiview Coding},
  year      = {2020},
  editor    = {Andrea Vedaldi and Horst Bischof and Thomas Brox and Jan{-}Michael Frahm},
  pages     = {776--794},
  series    = {Lecture Notes in Computer Science},
  volume    = {12356},
}

@inproceedings{wang2019compact,
 author = {Wang, Yining  and
Zhou, Long  and
Zhang, Jiajun  and
Zhai, Feifei  and
Xu, Jingfang  and
Zong, Chengqing},
 booktitle = {Proc. of ACL},
 pages = {1213--1223},
 title = {A Compact and Language-Sensitive Multilingual Translation Method},
 year = {2019}
}

@Article{wu2016googles,
  author  = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal = {arXiv preprint arXiv:1609.08144},
  title   = {Google's neural machine translation system: Bridging the gap between human and machine translation},
  year    = {2016},
}

@article{wu2020clear,
 author = {Zhuofeng Wu and
Sinong Wang and
Jiatao Gu and
Madian Khabsa and
Fei Sun and
Hao Ma},
 eprint = {2012.15466},
 journal = {CoRR},
 title = {{CLEAR:} Contrastive Learning for Sentence Representation},
 volume = {abs/2012.15466},
 year = {2020}
}

@article{zhu2021serial,
 author = {Zhu, Yaoming and Feng, Jiangtao and Zhao, Chengqi and Wang, Mingxuan and Li, Lei},
 journal = {arXiv preprint arXiv:2104.08154},
 title = {Serial or Parallel? Plug-able Adapter for multilingual machine translation},
 year = {2021}
}

@InProceedings{zhuang2019local,
  author    = {Chengxu Zhuang and Alex Lin Zhai and Daniel Yamins},
  booktitle = {Proc. of ICCV},
  title     = {Local Aggregation for Unsupervised Learning of Visual Embeddings},
  year      = {2019},
  pages     = {6001--6011},
}
@inproceedings{alinejad2020effectively,
 author = {Alinejad, Ashkan  and
Sarkar, Anoop},
 booktitle = {Proc. of EMNLP},
 pages = {8014--8020},
 title = {Effectively pretraining a speech translation decoder with Machine Translation data},
 year = {2020}
}

@InProceedings{baevski2020vq,
  author    = {Alexei Baevski and Steffen Schneider and Michael Auli},
  booktitle = {Proc. of ICLR},
  title     = {vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations},
  year      = {2020},
}

@InProceedings{berard2016listena,
  author    = {B{\'e}rard, Alexandre and Pietquin, Olivier and Besacier, Laurent and Servan, Christophe},
  booktitle = {NIPS Workshop on end-to-end learning for speech and audio processing},
  title     = {Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text Translation},
  year      = {2016},
}

@Article{kano2018structured,
  author        = {Takatomo Kano and Sakriani Sakti and Satoshi Nakamura},
  title         = {Structured-based Curriculum Learning for End-to-end English-Japanese Speech Translation},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1802.06003},
  primaryclass  = {cs.CL},
}

@inproceedings{kudo2018sentencepiece,
 author = {Kudo, Taku  and
Richardson, John},
 booktitle = {Proc. of EMNLP},
 pages = {66--71},
 title = {{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
 year = {2018}
}

@inproceedings{lewis2020bart,
 author = {Lewis, Mike  and
Liu, Yinhan  and
Goyal, Naman  and
Ghazvininejad, Marjan  and
Mohamed, Abdelrahman  and
Levy, Omer  and
Stoyanov, Veselin  and
Zettlemoyer, Luke},
 booktitle = {Proc. of ACL},
 pages = {7871--7880},
 title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
 year = {2020}
}

@InProceedings{nguyen2020investigating,
  author    = {Nguyen, Ha and Bougares, Fethi and Tomashenko, Natalia and Est{\`e}ve, Yannick and Besacier, Laurent},
  booktitle = {Interspeech},
  title     = {Investigating Self-supervised Pre-training for End-to-end Speech Translation},
  year      = {2020},
}

@inproceedings{panayotov2015librispeech,
 author = {Vassil Panayotov and
Guoguo Chen and
Daniel Povey and
Sanjeev Khudanpur},
 booktitle = {2015 {IEEE} International Conference on Acoustics, Speech and Signal
Processing, {ICASSP} 2015, South Brisbane, Queensland, Australia,
April 19-24, 2015},
 pages = {5206--5210},
 title = {Librispeech: An {ASR} corpus based on public domain audio books},
 year = {2015}
}

@Article{pino2020selfa,
  author  = {Pino, Juan and Xu, Qiantong and Ma, Xutai and Dousti, Mohammad Javad and Tang, Yun},
  journal = {Interspeech},
  title   = {Self-Training for End-to-End Speech Translation},
  year    = {2020},
  pages   = {1476--1480},
}

@inproceedings{schneider2019wav2vec,
 author = {Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
 booktitle = {INTERSPEECH},
 title = {wav2vec: Unsupervised Pre-Training for Speech Recognition.},
 year = {2019}
}

@article{tang2020multilingual,
 author = {Tang, Yuqing and Tran, Chau and Li, Xian and Chen, Peng-Jen and Goyal, Naman and Chaudhary, Vishrav and Gu, Jiatao and Fan, Angela},
 journal = {arXiv preprint arXiv:2008.00401},
 title = {Multilingual translation with extensible multilingual pretraining and finetuning},
 year = {2020}
}

@InProceedings{li2021multilingual,
  author    = {Xian Li and Changhan Wang and Yun Tang and Chau Tran and Yuqing Tang and Juan Pino and Alexei Baevski and Alexis Conneau and Michael Auli},
  booktitle = {Proc. of ACL},
  title     = {Multilingual Speech Translation with Efficient Finetuning of Pretrained Models},
  year      = {2021},
}

@InProceedings{wang2021unispeech,
  author    = {Wang, Chengyi and Wu, Yu and Qian, Yao and Kumatani, Kenichi and Liu, Shujie and Wei, Furu and Zeng, Michael and Huang, Xuedong},
  booktitle = {Proc. of ICML},
  title     = {UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data},
  year      = {2021},
}

@InProceedings{wu2020self,
  author    = {Wu, Anne and Wang, Changhan and Pino, Juan and Gu, Jiatao},
  booktitle = {Interspeech},
  title     = {Self-supervised representations improve end-to-end speech translation},
  year      = {2020},
}

@article{yi2021applying,
 archiveprefix = {arXiv},
 author = {Cheng Yi and Jianzhong Wang and Ning Cheng and Shiyu Zhou and Bo Xu},
 eprint = {2012.12121},
 primaryclass = {cs.CL},
 title = {Applying Wav2vec2.0 to Speech Recognition in Various Low-resource Languages},
 year = {2021}
}

@InProceedings{zheng2021fused,
  author        = {Renjie Zheng and Junkun Chen and Mingbo Ma and Liang Huang},
  booktitle     = {Proc. of ICML},
  title         = {Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2102.05766},
  primaryclass  = {cs.CL},
}

@Comment{jabref-meta: databaseType:bibtex;}
